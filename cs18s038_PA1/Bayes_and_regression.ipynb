{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Instructions to students:\n",
    "\n",
    "1. There are 5 types of cells in this notebook. The cell type will be indicated within the cell.\n",
    "    1. Markdown cells with problem written in it. (DO NOT TOUCH THESE CELLS) (**Cell type: TextRead**)\n",
    "    2. Python cells with setup code for further evaluations. (DO NOT TOUCH THESE CELLS) (**Cell type: CodeRead**)\n",
    "    3. Python code cells with some template code or empty cell. (FILL CODE IN THESE CELLS BASED ON INSTRUCTIONS IN CURRENT AND PREVIOUS CELLS) (**Cell type: CodeWrite**)\n",
    "    4. Markdown cells where a written reasoning or conclusion is expected. (WRITE SENTENCES IN THESE CELLS) (**Cell type: TextWrite**)\n",
    "    5. Temporary code cells for convenience and TAs. (YOU MAY DO WHAT YOU WILL WITH THESE CELLS, TAs WILL REPLACE WHATEVER YOU WRITE HERE WITH OFFICIAL EVALUATION CODE) (**Cell type: Convenience**)\n",
    "    \n",
    "2. You are not allowed to insert new cells in the submitted notebook.\n",
    "\n",
    "3. You are not allowed to import any extra packages.\n",
    "\n",
    "4. The code is to be written in Python 3.6 syntax. Latest versions of other packages maybe assumed.\n",
    "\n",
    "5. In CodeWrite Cells, the only outputs to be given are plots asked in the question. Nothing else to be output/print. \n",
    "\n",
    "6. If TextWrite cells ask you to give accuracy/error/other numbers you can print them on the code cells, but remove the print statements before submitting.\n",
    "\n",
    "7. The convenience code can be used to check the expected syntax of the functions. At a minimum, your entire notebook must run with \"run all\" with the convenience cells as it is. Any runtime failures on the submitted notebook as it is will get zero marks.\n",
    "\n",
    "8. All code must be written by yourself. Copying from other students/material on the web is strictly prohibited. Any violations will result in zero marks. \n",
    "\n",
    "9. You may discuss broad ideas with friends, but all code must be written by yourself.\n",
    "\n",
    "9. All datasets will be given as .npz files, and will contain data in 4 numpy arrays :\"X_train, Y_train, X_test, Y_test\". In that order. The meaning of the 4 arrays can be easily inferred from their names.\n",
    "\n",
    "10. All plots must be labelled properly, all tables must have rows and columns named properly.\n",
    "\n",
    "11. Plotting the data and prediction is highly encouraged for debugging. But remove debugging/understanding code before submitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type : CodeRead\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# Problem 1: Learning Binary Bayes Classifiers from data with Max. Likelihood \n",
    "\n",
    "Derive Bayes classifiers under assumptions below, and use ML estimators to compute and return the results on a test set. \n",
    "\n",
    "1a) Assume $X|Y=-1 \\sim \\mathcal{N}(\\mu_-, I)$ and  $X|Y=1 \\sim \\mathcal{N}(\\mu_+, I)$. *(Same known covariance)*\n",
    "\n",
    "1b) Assume $X|Y=-1 \\sim \\mathcal{N}(\\mu_-, \\Sigma)$ and $X|Y=1 \\sim \\mathcal{N}(\\mu_+, \\Sigma)$ *(Same unknown covariance)*\n",
    "\n",
    "1c) Assume $X|Y=-1 \\sim \\mathcal{N}(\\mu_-, \\Sigma_-)$ and $X|Y=1 \\sim \\mathcal{N}(\\mu_+, \\Sigma_+)$ *(different unknown covariance)*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import math\n",
    "\n",
    "def probability_function(data_point,mu,sigma):\n",
    "    x_minus_mu = np.subtract(data_point,mu)\n",
    "    x_minus_mu_transpose = np.subtract(data_point,mu).T\n",
    "    sigma_inverse = np.linalg.inv(sigma)\n",
    "    product = np.dot(x_minus_mu_transpose,sigma_inverse)\n",
    "    liklihood =(1.0/(np.sqrt(np.linalg.det(sigma))))*math.exp(-0.5*np.dot(product,x_minus_mu ))\n",
    "    return liklihood\n",
    "\n",
    "\n",
    "def Bayes1a(X_train, Y_train, X_test):\n",
    "    x_train_pos = []\n",
    "    x_train_neg = []\n",
    "    \n",
    "    for (b, c) in zip(X_train, Y_train):\n",
    "        \n",
    "        if c == 1:\n",
    "            x_train_pos.append(b)\n",
    "        else:\n",
    "            x_train_neg.append(b)\n",
    "            \n",
    "    x_train_pos_mean_vector = np.mean(x_train_pos, axis=0)\n",
    "    \n",
    "    x_train_neg_mean_vector = np.mean(x_train_neg, axis=0)\n",
    "    \n",
    "    prior_for_class_plus_one = float(len(x_train_pos))/float(len(X_train))\n",
    "    \n",
    "    prior_for_class_minus_one = float(len(x_train_neg))/float(len(X_train))\n",
    "    \n",
    "    sigma = np.identity(len(X_train[0]))\n",
    "\n",
    "    Y_test_pred = []\n",
    "    \n",
    "    for sample in np.array(X_test):\n",
    "        probability_for_class_plus_one = probability_function(sample,x_train_pos_mean_vector,\n",
    "                                                              sigma)*prior_for_class_plus_one\n",
    "        \n",
    "        probability_for_class_minus_one = probability_function(sample,x_train_neg_mean_vector,\n",
    "                                                              sigma)*prior_for_class_minus_one\n",
    "        \n",
    "        if probability_for_class_plus_one >= probability_for_class_minus_one :\n",
    "            Y_test_pred.append(1)\n",
    "            \n",
    "        else:\n",
    "            Y_test_pred.append(-1)\n",
    "            \n",
    "    return Y_test_pred\n",
    "\n",
    "    \"\"\" Give prediction for test instance using assumption 1a.\n",
    "\n",
    "    \n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: +1/-1 numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : +1/-1 numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Bayes1b(X_train, Y_train, X_test):\n",
    "    \"\"\" Give prediction for test instance using assumption 1b.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: +1/-1 numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : +1/-1 numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    x_train_pos = []\n",
    "    x_train_neg = []\n",
    "    \n",
    "    for (b, c) in zip(X_train, Y_train):\n",
    "        \n",
    "        if c == 1:\n",
    "            x_train_pos.append(b)\n",
    "        else:\n",
    "            x_train_neg.append(b)\n",
    "            \n",
    "    x_train_pos_mean_vector = np.mean(x_train_pos, axis=0)\n",
    "    x_train_neg_mean_vector = np.mean(x_train_neg, axis=0)\n",
    "    \n",
    "    prior_for_class_plus_one = float(len(x_train_pos))/float(len(X_train))\n",
    "    prior_for_class_minus_one = float(len(x_train_neg))/float(len(X_train))\n",
    "    \n",
    "    sigma = np.cov(X_train.T)\n",
    "    \n",
    "\n",
    "    Y_test_pred = []\n",
    "    \n",
    "    for sample in np.array(X_test):\n",
    "        probability_for_class_plus_one = probability_function(sample, x_train_pos_mean_vector,\n",
    "                                                              sigma) * prior_for_class_plus_one\n",
    "        \n",
    "        probability_for_class_minus_one = probability_function(sample, x_train_neg_mean_vector,\n",
    "                                                               sigma) * prior_for_class_minus_one\n",
    "        \n",
    "        if probability_for_class_plus_one >= probability_for_class_minus_one:\n",
    "            Y_test_pred.append(1)\n",
    "        else:\n",
    "            Y_test_pred.append(-1)\n",
    "            \n",
    "    return Y_test_pred\n",
    "\n",
    "\n",
    "def Bayes1c(X_train, Y_train, X_test):\n",
    "    \"\"\" Give prediction for test instance using assumption 1c.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: +1/-1 numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : +1/-1 numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    x_train_pos = []\n",
    "    x_train_neg = []\n",
    "    \n",
    "    for (b, c) in zip(X_train, Y_train):\n",
    "        if c == 1:\n",
    "            x_train_pos.append(b)\n",
    "        else:\n",
    "            x_train_neg.append(b)\n",
    "            \n",
    "    x_train_pos_mean_vector = np.mean(x_train_pos, axis=0)\n",
    "    x_train_neg_mean_vector = np.mean(x_train_neg, axis=0)\n",
    "    \n",
    "    prior_for_class_plus_one = float(len(x_train_pos))/float(len(X_train))\n",
    "    prior_for_class_minus_one = float(len(x_train_neg))/float(len(X_train))\n",
    "    \n",
    "    sigma_positive = np.cov(np.array(x_train_pos).T)\n",
    "    sigma_negative = np.cov(np.array(x_train_neg).T)\n",
    "\n",
    "\n",
    "    Y_test_pred = []\n",
    "    for sample in np.array(X_test):\n",
    "        \n",
    "        probability_for_class_plus_one = probability_function(sample, x_train_pos_mean_vector, \n",
    "                                                              sigma_positive) * prior_for_class_plus_one\n",
    "        \n",
    "        probability_for_class_minus_one = probability_function(sample, x_train_neg_mean_vector,\n",
    "                                                               sigma_negative) * prior_for_class_minus_one\n",
    "        \n",
    "        if probability_for_class_plus_one >= probability_for_class_minus_one:\n",
    "            Y_test_pred.append(1)\n",
    "        else:\n",
    "            Y_test_pred.append(-1)\n",
    "            \n",
    "    return Y_test_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Cell type : Convenience\n",
    "\n",
    "# Testing the functions above\n",
    "\n",
    "# To TAs: Replace this cell with the testing cell developed.\n",
    "\n",
    "# To students: You may use the example here for testing syntax issues \n",
    "# with your functions, and also as a sanity check. But the final evaluation\n",
    "# will be done for different inputs to the functions. (So you can't just \n",
    "# solve the problem for this one example given below.) \n",
    "\n",
    "\n",
    "# X_train_pos = np.random.randn(1000,2)+np.array([[1.,2.]])\n",
    "# X_train_neg = np.random.randn(1000,2)+np.array([[2.,4.]])\n",
    "# X_train = np.concatenate((X_train_pos, X_train_neg), axis=0)\n",
    "# Y_train = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))\n",
    "# X_test_pos = np.random.randn(1000,2)+np.array([[1.,2.]])\n",
    "# X_test_neg = np.random.randn(1000,2)+np.array([[2.,4.]])\n",
    "# X_test = np.concatenate((X_test_pos, X_test_neg), axis=0)\n",
    "# Y_test = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))\n",
    "\n",
    "# Y_pred_test_1a = Bayes1a(X_train, Y_train, X_test)\n",
    "# Y_pred_test_1b = Bayes1b(X_train, Y_train, X_test)\n",
    "# Y_pred_test_1c = Bayes1c(X_train, Y_train, X_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# Problem 1\n",
    "\n",
    "1d) Run the above three algorithms (Bayes1a,1b and 1c), for the three datasets given (dataset1_1.npz, dataset1_2.npz, dataset1_3.npz) in the cell below.\n",
    "\n",
    "In the next CodeWrite cell, Plot all the classifiers (3 classification algos on 3 datasets = 9 plots) on a 2d plot (color the positively classified area light green, and negatively classified area light red). Add the training data points also on the plot. Plots to be organised into 3 plots follows: One plot for each dataset, with three subplots in each for the three classifiers. Label the 9 plots appropriately. \n",
    "\n",
    "In the next Textwrite cell, summarise (use the plots of the data and the assumptions in the problem to explain) your observations regarding the six learnt classifiers, and also give the error rate of the three classifiers on the three datasets as 3x3 table, with appropriately named rows and columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1f83b2fff373>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_two\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_two\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tomato'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'o'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY_train_two\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\MachineLearningAssignment\\venv\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, data, **kwargs)\u001b[0m\n\u001b[0;32m   2862\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2863\u001b[0m         verts=verts, edgecolors=edgecolors, **({\"data\": data} if data\n\u001b[1;32m-> 2864\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2865\u001b[0m     \u001b[0msci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2866\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\MachineLearningAssignment\\venv\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1810\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32m~\\PycharmProjects\\MachineLearningAssignment\\venv\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[0;32m   4325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4326\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4327\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoscale_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4329\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcollection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\MachineLearningAssignment\\venv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mautoscale_view\u001b[1;34m(self, tight, scalex, scaley)\u001b[0m\n\u001b[0;32m   2500\u001b[0m         handle_single_axis(\n\u001b[0;32m   2501\u001b[0m             \u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_autoscaleXon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shared_x_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'intervalx'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2502\u001b[1;33m             'minposx', self.xaxis, self._xmargin, x_stickies, self.set_xbound)\n\u001b[0m\u001b[0;32m   2503\u001b[0m         handle_single_axis(\n\u001b[0;32m   2504\u001b[0m             \u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_autoscaleYon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shared_y_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'intervaly'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\MachineLearningAssignment\\venv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mhandle_single_axis\u001b[1;34m(scale, autoscaleon, shared_axes, interval, minpos, axis, margin, stickies, set_bound)\u001b[0m\n\u001b[0;32m   2481\u001b[0m             \u001b[0mx0t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2483\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1t\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2484\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx1t\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx0t\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmargin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2485\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell type : CodeWrite\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# write the code for loading the data, running the three algos, and plotting here. \n",
    "# (Use the functions written previously.)\n",
    "with np.load('dataset1_1.npz') as data:\n",
    "    X_train_one = data['arr_0']\n",
    "    Y_train_one = data['arr_1']\n",
    "    X_test_one = data['arr_2']\n",
    "    Y_test_one = data['arr_3']\n",
    "\n",
    "y_predicted11a = Bayes1a(X_train_one,Y_train_one,X_test_one)\n",
    "y_predicted11b = Bayes1b(X_train_one,Y_train_one,X_test_one)\n",
    "y_predicted11c = Bayes1c(X_train_one,Y_train_one,X_test_one)\n",
    "\n",
    "\n",
    "plt.figure(1,figsize=(10,5))\n",
    "\n",
    "plt.subplot(131)\n",
    "\n",
    "length_of_dataset_one = len(X_test_one)\n",
    "\n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (y_predicted11a[data] == 1):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='tomato', marker='o' )\n",
    "    \n",
    "    if (Y_train_one[data] == 1):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='tomato', marker='o' )\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 1a\",fontsize=10)\n",
    "        \n",
    "    \n",
    "    \n",
    "plt.subplot(132)\n",
    "\n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (y_predicted11b[data] == 1):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='tomato', marker='o' )   \n",
    "\n",
    "    if (Y_train_one[data] == 1):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='tomato', marker='o' )\n",
    "        \n",
    "        \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 1b \",fontsize=10)\n",
    "        \n",
    "    \n",
    "plt.subplot(133)\n",
    "\n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (y_predicted11c[data] == 1):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='tomato', marker='o' ) \n",
    "    \n",
    "    if (Y_train_one[data] == 1):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='tomato', marker='o' )\n",
    "    \n",
    "        \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 1c \",fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##############################################################\n",
    "with np.load('dataset1_2.npz') as data:\n",
    "    X_train_two = data['arr_0']\n",
    "    Y_train_two = data['arr_1']\n",
    "    X_test_two= data['arr_2']\n",
    "    Y_test_two= data['arr_3']\n",
    "\n",
    "y_predicted12a = Bayes1a(X_train_two,Y_train_two,X_test_two)\n",
    "y_predicted12b = Bayes1b(X_train_two,Y_train_two,X_test_two)\n",
    "y_predicted12c = Bayes1c(X_train_two,Y_train_two,X_test_two)\n",
    "\n",
    "    \n",
    "plt.figure(2,figsize=(10,5))\n",
    "plt.subplot(131)\n",
    "\n",
    "length_of_dataset_two = len(X_test_two)\n",
    "\n",
    "for data in range(length_of_dataset_two):\n",
    "    \n",
    "    if (y_predicted12a[data] == 1):\n",
    "        plt.scatter(X_test_two[data][0], X_test_two[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_test_two[data][0], X_test_two[data][1], c='tomato', marker='o' )\n",
    "               \n",
    "    if (Y_train_two[data] == 1):\n",
    "        plt.scatter(X_train_two[data][0], X_train_two[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_train_two[data][0], X_train_two[data][1], c='tomato', marker='o' )\n",
    "        \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 2a \",fontsize=10)\n",
    "\n",
    "plt.subplot(132)\n",
    "\n",
    "for data in range(length_of_dataset_two):\n",
    "    \n",
    "    if (y_predicted12b[data] == 1):\n",
    "        plt.scatter(X_test_two[data][0], X_test_two[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_test_two[data][0], X_test_two[data][1], c='tomato', marker='o' ) \n",
    "        \n",
    "    if (Y_train_two[data] == 1):\n",
    "        plt.scatter(X_train_two[data][0], X_train_two[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_train_two[data][0], X_train_two[data][1], c='tomato', marker='o' )\n",
    "        \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 2b \",fontsize=10)\n",
    "\n",
    "plt.subplot(133)\n",
    "\n",
    "for data in range(length_of_dataset_two):\n",
    "    \n",
    "    if (y_predicted12c[data] == 1):\n",
    "        plt.scatter(X_test_two[data][0], X_test_two[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_test_two[data][0], X_test_two[data][1], c='tomato', marker='o' ) \n",
    "        \n",
    "        \n",
    "    if (Y_train_two[data] == 1):\n",
    "        plt.scatter(X_train_two[data][0], X_train_two[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_train_two[data][0], X_train_two[data][1], c='tomato', marker='o' )\n",
    "\n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 2c\",fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "with np.load('dataset1_3.npz') as data:\n",
    "    X_train_three = data['arr_0']\n",
    "    Y_train_three = data['arr_1']\n",
    "    X_test_three = data['arr_2']\n",
    "    Y_test_three = data['arr_3'] \n",
    "\n",
    "y_predicted13a = Bayes1a(X_train_three,Y_train_three,X_test_three)\n",
    "y_predicted13b = Bayes1b(X_train_three,Y_train_three,X_test_three)\n",
    "y_predicted13c = Bayes1c(X_train_three,Y_train_three,X_test_three)\n",
    "    \n",
    "plt.figure(3,figsize=(10,5))\n",
    "\n",
    "plt.subplot(131)\n",
    "\n",
    "length_of_dataset_three = len(X_test_three)\n",
    "\n",
    "for data in range(length_of_dataset_three):\n",
    "    \n",
    "    if (y_predicted13a[data] == 1):\n",
    "        plt.scatter(X_test_three[data][0], X_test_three[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_test_three[data][0], X_test_three[data][1], c='tomato', marker='o' )   \n",
    "    \n",
    "    if (Y_train_three[data] == 1):\n",
    "        plt.scatter(X_train_three[data][0], X_train_three[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_train_three[data][0], X_train_three[data][1], c='tomato', marker='o' )\n",
    "        \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 1a\",fontsize=10)\n",
    "\n",
    "plt.subplot(132)\n",
    "\n",
    "for data in range(length_of_dataset_three):\n",
    "    \n",
    "    if (y_predicted13b[data] == 1):\n",
    "        plt.scatter(X_test_three[data][0], X_test_three[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_test_three[data][0], X_test_three[data][1], c='tomato', marker='o' ) \n",
    "        \n",
    "    if (Y_train_three[data] == 1):\n",
    "        plt.scatter(X_train_three[data][0], X_train_three[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_train_three[data][0], X_train_three[data][1],c='tomato', marker='o' )\n",
    "        \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 2b \",fontsize=10)\n",
    "\n",
    "plt.subplot(133)\n",
    "\n",
    "for data in range(length_of_dataset_three):\n",
    "    \n",
    "    if (y_predicted13c[data] == 1):\n",
    "        plt.scatter(X_test_three[data][0], X_test_three[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_test_three[data][0], X_test_three[data][1], c='tomato', marker='o' ) \n",
    "        \n",
    "    if (Y_train_three[data] == 1):\n",
    "        plt.scatter(X_train_three[data][0], X_train_three[data][1], c='lightgreen', marker='o' )\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X_train_three[data][0], X_train_three[data][1], c='tomato', marker='o' )\n",
    "        \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 3c \",fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextWrite ** \n",
    "(Write your observations and table of errors here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** Cell type : TextRead ** \n",
    "\n",
    "\n",
    "# Problem 2 : Learning Multiclass Bayes Classifiers from data with Max. Likeli.\n",
    "\n",
    "Derive Bayes classifiers under assumptions below, and use ML estimators to compute and return the results on a test set. The $4\\times 4$ loss matrix giving the loss incurred for predicting $i$ when truth is $j$ is below.\n",
    "\n",
    "$L=\\begin{bmatrix} 0 &1 & 2& 3\\\\ 1 &0 & 1& 2\\\\ 2 &1 & 0& 1\\\\ 3 &2 & 1& 0 \\end{bmatrix}$ \n",
    "\n",
    "2a) Assume $X|Y=a$ is distributed as Normal with mean $\\mu_a$ and variance $I$.\n",
    "\n",
    "2b) Assume $X|Y=a$ is distributed as Normal with mean $\\mu_a$ and variance $\\Sigma$.\n",
    "\n",
    "2c) Assume $X|Y=a$ is distributed as Normal with mean $\\mu_a$ and variance $\\Sigma_a$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type : CodeWrite\n",
    "# Fill in functions in this cell\n",
    "\n",
    "import  numpy as np\n",
    "def Bayes2a(X_train, Y_train, X_test):\n",
    "    \"\"\" Give Bayes classifier prediction for test instances \n",
    "    using assumption 2a.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: {1,2,3,4} numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : {1,2,3,4} numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    loss_matrix = [[0,1,2,3],\n",
    "                   [1,0,1,2],\n",
    "                   [2,1,0,1],\n",
    "                   [3,2,1,0]]\n",
    "    \n",
    "    x_train_class_1 = []\n",
    "    x_train_class_2 = []\n",
    "    x_train_class_3 = []\n",
    "    x_train_class_4 = []\n",
    "    \n",
    "    for (b, c) in zip(X_train, Y_train):\n",
    "        if c == 1:\n",
    "            x_train_class_1.append(b)\n",
    "        elif c == 2:\n",
    "            x_train_class_2.append(b)\n",
    "        elif c == 3:\n",
    "            x_train_class_3.append(b)\n",
    "        elif c == 4:\n",
    "            x_train_class_4.append(b)\n",
    "            \n",
    "    x_train_class_1_mean = np.mean(x_train_class_1, axis=0)\n",
    "    \n",
    "    x_train_class_2_mean = np.mean(x_train_class_2, axis=0)\n",
    "    \n",
    "    x_train_class_3_mean = np.mean(x_train_class_3, axis=0)\n",
    "    x_train_class_4_mean = np.mean(x_train_class_4, axis=0)\n",
    "    \n",
    "    prior_for_class_1 = float(len(x_train_class_1)) / float(len(X_train))\n",
    "    \n",
    "    prior_for_class_2 = float(len(x_train_class_2)) / float(len(X_train))\n",
    "    \n",
    "    prior_for_class_3 = float(len(x_train_class_3)) / float(len(X_train))\n",
    "    prior_for_class_4 = float(len(x_train_class_4)) / float(len(X_train))\n",
    "    \n",
    "    covariance_for_data = np.identity(len(X_train[0]))\n",
    "    \n",
    "    Y_test_pred = []\n",
    "    \n",
    "    for test_data in np.array(X_test):\n",
    "        \n",
    "        probability_vector = []\n",
    "        \n",
    "        probability_for_class_1 = probability_function(test_data, x_train_class_1_mean,\n",
    "                                                       covariance_for_data)*prior_for_class_1\n",
    "        \n",
    "        probability_for_class_2 = probability_function(test_data, x_train_class_2_mean,\n",
    "                                                       covariance_for_data)*prior_for_class_2\n",
    "        \n",
    "        probability_for_class_3 = probability_function(test_data, x_train_class_3_mean,\n",
    "                                                       covariance_for_data)*prior_for_class_3\n",
    "        \n",
    "        probability_for_class_4 = probability_function(test_data, x_train_class_4_mean,\n",
    "                                                       covariance_for_data)*prior_for_class_4\n",
    "        \n",
    "        probability_vector.append( probability_for_class_1)\n",
    "        \n",
    "        probability_vector.append(probability_for_class_2 )\n",
    "        \n",
    "        probability_vector.append(probability_for_class_3 )\n",
    "        \n",
    "        probability_vector.append( probability_for_class_4)\n",
    "       \n",
    "        dot_product = np.dot( probability_vector,loss_matrix)\n",
    "\n",
    "        minimum_value = np.argmin(dot_product, axis=0 )\n",
    "        \n",
    "        Y_test_pred.append(minimum_value+1)\n",
    "        \n",
    "    return Y_test_pred\n",
    "    #sigma = np.cov(X_train.T)\n",
    "    \n",
    "    \n",
    "def Bayes2b(X_train, Y_train, X_test):\n",
    "    \"\"\" Give Bayes classifier prediction for test instances \n",
    "    using assumption 2b.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: {1,2,3,4} numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : {1,2,3,4} numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    loss_matrix = [[0,1,2,3],\n",
    "                   [1,0,1,2],\n",
    "                   [2,1,0,1],\n",
    "                   [3,2,1,0]]\n",
    "    \n",
    "    x_train_class_1 = []\n",
    "    x_train_class_2 = []\n",
    "    x_train_class_3 = []\n",
    "    x_train_class_4 = []\n",
    "    \n",
    "    for (b, c) in zip(X_train, Y_train):\n",
    "        if c == 1:\n",
    "            x_train_class_1.append(b)\n",
    "        elif c == 2:\n",
    "            x_train_class_2.append(b)\n",
    "        elif c == 3:\n",
    "            x_train_class_3.append(b)\n",
    "        elif c == 4:\n",
    "            x_train_class_4.append(b)\n",
    "            \n",
    "    x_train_class_1_mean = np.mean(x_train_class_1, axis=0)\n",
    "    x_train_class_2_mean = np.mean(x_train_class_2, axis=0)\n",
    "    \n",
    "    x_train_class_3_mean = np.mean(x_train_class_3, axis=0)\n",
    "    \n",
    "    x_train_class_4_mean = np.mean(x_train_class_4, axis=0)\n",
    "    \n",
    "    prior_for_class_1 = float(len(x_train_class_1)) / float(len(X_train))\n",
    "    \n",
    "    prior_for_class_2 = float(len(x_train_class_2)) / float(len(X_train))\n",
    "           \n",
    "    prior_for_class_3 = float(len(x_train_class_3)) / float(len(X_train))\n",
    "    prior_for_class_4 = float(len(x_train_class_4)) / float(len(X_train))\n",
    "    \n",
    "    covariance_for_data = np.cov(np.array(X_train).T)\n",
    "    \n",
    "    Y_test_pred = []\n",
    "    \n",
    "    for test_data in np.array(X_test):\n",
    "        \n",
    "        probability_vector = []\n",
    "        \n",
    "        probability_for_class_1 = probability_function(test_data, x_train_class_1_mean,\n",
    "                                                       covariance_for_data)*prior_for_class_1\n",
    "        \n",
    "        probability_for_class_2 = probability_function(test_data, x_train_class_2_mean,\n",
    "                                                       covariance_for_data)*prior_for_class_2\n",
    "        \n",
    "        probability_for_class_3 = probability_function(test_data, x_train_class_3_mean,\n",
    "                                                       covariance_for_data)*prior_for_class_3\n",
    "        \n",
    "        probability_for_class_4 = probability_function(test_data, x_train_class_4_mean,\n",
    "                                                       covariance_for_data)*prior_for_class_4\n",
    "        \n",
    "        probability_vector.append(probability_for_class_1)\n",
    "        \n",
    "        probability_vector.append(probability_for_class_2)\n",
    "        \n",
    "        probability_vector.append(probability_for_class_3)\n",
    "        \n",
    "        probability_vector.append(probability_for_class_4)\n",
    "       \n",
    "        dot_product = np.dot(probability_vector,loss_matrix)\n",
    "\n",
    "        minimum_value = np.argmin(dot_product, axis=0)\n",
    "        \n",
    "        Y_test_pred.append(minimum_value+1)\n",
    "        \n",
    "    return Y_test_pred\n",
    "\n",
    "def Bayes2c(X_train, Y_train, X_test):\n",
    "    \"\"\" Give Bayes classifier prediction for test instances \n",
    "    using assumption 2c.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: {1,2,3,4} numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : {1,2,3,4} numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    loss_matrix = [[0,1,2,3],\n",
    "                   [1,0,1,2],\n",
    "                   [2,1,0,1],\n",
    "                   [3,2,1,0]]\n",
    "    \n",
    "    x_train_class_1 = []\n",
    "    x_train_class_2 = []\n",
    "    x_train_class_3 = []\n",
    "    x_train_class_4 = []\n",
    "    \n",
    "    for (b, c) in zip(X_train, Y_train):\n",
    "        if c == 1:\n",
    "            x_train_class_1.append(b)\n",
    "        elif c == 2:\n",
    "            x_train_class_2.append(b)\n",
    "        elif c == 3:\n",
    "            x_train_class_3.append(b)\n",
    "        elif c == 4:\n",
    "            x_train_class_4.append(b)\n",
    "            \n",
    "    x_train_class_1_mean = np.mean(x_train_class_1, axis=0)\n",
    "    \n",
    "    x_train_class_2_mean = np.mean(x_train_class_2, axis=0)\n",
    "    x_train_class_3_mean = np.mean(x_train_class_3, axis=0)\n",
    "    \n",
    "    x_train_class_4_mean = np.mean(x_train_class_4, axis=0)\n",
    "    \n",
    "    prior_for_class_1 = float(len(x_train_class_1)) / float(len(X_train))\n",
    "    \n",
    "    prior_for_class_2 = float(len(x_train_class_2)) / float(len(X_train))\n",
    "    prior_for_class_3 = float(len(x_train_class_3)) / float(len(X_train))\n",
    "    prior_for_class_4 = float(len(x_train_class_4)) / float(len(X_train))\n",
    "    \n",
    "    covariance_for_data_of_class_1 = np.cov(np.array(x_train_class_1).T)\n",
    "    \n",
    "    covariance_for_data_of_class_2 = np.cov(np.array(x_train_class_2).T)\n",
    "    \n",
    "    covariance_for_data_of_class_3 = np.cov(np.array(x_train_class_3).T)\n",
    "    \n",
    "    \n",
    "    covariance_for_data_of_class_4 = np.cov(np.array(x_train_class_4).T)\n",
    "    \n",
    "    \n",
    "    Y_test_pred = []\n",
    "    \n",
    "    for test_data in np.array(X_test):\n",
    "        probability_vector = []\n",
    "        \n",
    "        probability_for_class_1 = probability_function(test_data, x_train_class_1_mean,\n",
    "                                                       covariance_for_data_of_class_1)*prior_for_class_1\n",
    "        \n",
    "        probability_for_class_2 = probability_function(test_data, x_train_class_2_mean,\n",
    "                                                       covariance_for_data_of_class_2)*prior_for_class_2\n",
    "        \n",
    "        probability_for_class_3 = probability_function(test_data, x_train_class_3_mean,\n",
    "                                                       covariance_for_data_of_class_3)*prior_for_class_3\n",
    "        \n",
    "        probability_for_class_4 = probability_function(test_data, x_train_class_4_mean,\n",
    "                                                       covariance_for_data_of_class_4)*prior_for_class_4\n",
    "        \n",
    "        probability_vector.append(probability_for_class_1)\n",
    "        \n",
    "        probability_vector.append(probability_for_class_2)\n",
    "        \n",
    "        probability_vector.append(probability_for_class_3)\n",
    "        \n",
    "        probability_vector.append(probability_for_class_4)\n",
    "       \n",
    "        dot_product = np.dot(probability_vector,loss_matrix)\n",
    "\n",
    "        minimum_value = np.argmin(dot_product, axis=0)\n",
    "        Y_test_pred.append(minimum_value+1)\n",
    "    #print(Y_test_pred)\n",
    "    return Y_test_pred\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type : Convenience\n",
    "\n",
    "# Testing the functions above\n",
    "\n",
    "# Data 1\n",
    "import numpy as np\n",
    "\n",
    "# mat1=np.array([[1.,0.],[0.,1.]])\n",
    "# mat2=np.array([[1.,0.],[0.,1.]])\n",
    "# mat3=np.array([[1.,0.],[0.,1.]])\n",
    "# mat4=np.array([[1.,0.],[0.,1.]])\n",
    "\n",
    "# X_train_1 = np.dot(np.random.randn(1000,2), mat1)+np.array([[0.,0.]])\n",
    "# X_train_2 = np.dot(np.random.randn(1000,2), mat2)+np.array([[0.,2.]])\n",
    "# X_train_3 = np.dot(np.random.randn(1000,2), mat3)+np.array([[2.,0.]])\n",
    "# X_train_4 = np.dot(np.random.randn(1000,2), mat4)+np.array([[2.,2.]])\n",
    "# \n",
    "# X_train = np.concatenate((X_train_1, X_train_2, X_train_3, X_train_4), axis=0)\n",
    "# Y_train = np.concatenate(( np.ones(1000), 2*np.ones(1000), 3*np.ones(1000), 4*np.ones(1000) ))\n",
    "# \n",
    "# \n",
    "# X_test_1 = np.dot(np.random.randn(1000,2), mat1)+np.array([[0.,0.]])\n",
    "# X_test_2 = np.dot(np.random.randn(1000,2), mat2)+np.array([[0.,2.]])\n",
    "# X_test_3 = np.dot(np.random.randn(1000,2), mat3)+np.array([[2.,0.]])\n",
    "# X_test_4 = np.dot(np.random.randn(1000,2), mat4)+np.array([[2.,2.]])\n",
    "# \n",
    "# X_test = np.concatenate((X_test_1, X_test_2, X_test_3, X_test_4), axis=0)\n",
    "# Y_test = np.concatenate(( np.ones(1000), 2*np.ones(1000), 3*np.ones(1000), 4*np.ones(1000) ))\n",
    "\n",
    "\n",
    "#Y_pred_test_2a = Bayes2a(X_train, Y_train, X_test)\n",
    "#Y_pred_test_2b = Bayes2b(X_train, Y_train, X_test)\n",
    "#Y_pred_test_2c = Bayes2c(X_train, Y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# Problem 2\n",
    "\n",
    "2d) Run the above three algorithms (Bayes2a,2b and 2c), for the two datasets given (dataset2_1.npz, dataset2_2.npz) in the cell below.\n",
    "\n",
    "In the next CodeWrite cell, Plot all the classifiers (3 classification algos on 2 datasets = 6 plots) on a 2d plot (color the 4 areas classified as 1,2,3 and 4 differently). Add the training data points also on the plot. Plots to be organised as follows: One plot for each dataset, with three subplots in each for the three classifiers. Label the 6 plots appropriately. \n",
    "\n",
    "In the next Textwrite cell, summarise your observations regarding the six learnt classifiers. Give the *expected loss* (use the Loss matrix given in the problem.) of the three classifiers on the two datasets as 2x3 table, with appropriately named rows and columns. Also, give the 4x4 confusion matrix of the final classifier for all three algorithms and both datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type : CodeWrite\n",
    "# write the code for loading the data, running the three algos, and plotting here. \n",
    "# (Use the functions written previously.)\n",
    "import numpy as np\n",
    "\n",
    "with np.load('dataset2_1.npz') as data:\n",
    "    X_train_one = data['arr_0']\n",
    "    Y_train_one = data['arr_1']\n",
    "    X_test_one = data['arr_2']\n",
    "    Y_test_one = data['arr_3']\n",
    "\n",
    "y_predicted21a = Bayes2a(X_train_one,Y_train_one,X_test_one)\n",
    "y_predicted21b = Bayes2b(X_train_one,Y_train_one,X_test_one)\n",
    "y_predicted21c = Bayes2c(X_train_one,Y_train_one,X_test_one)\n",
    "\n",
    "\n",
    "plt.figure(4,figsize=(10,5))\n",
    "\n",
    "plt.subplot(131)\n",
    "\n",
    "length_of_dataset_one = len(X_test_one)\n",
    "\n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (y_predicted21a[data] == 1):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='r', marker='o' )\n",
    "    \n",
    "    elif (y_predicted21a[data] == 2):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (y_predicted21a[data] == 3):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (y_predicted21a[data] == 4):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='y', marker='o' )\n",
    "        \n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (Y_train_one[data] == 1):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='r', marker='o' )\n",
    "        \n",
    "    elif (Y_train_one[data] == 2):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 3):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 4):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='y', marker='o' )\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 2a\",fontsize=10)\n",
    "        \n",
    "    \n",
    "    \n",
    "plt.subplot(132)\n",
    "\n",
    "length_of_dataset_one = len(X_test_one)\n",
    "\n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (y_predicted21b[data] == 1):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='r', marker='o' )\n",
    "    \n",
    "    elif (y_predicted21b[data] == 2):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (y_predicted21b[data] == 3):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (y_predicted21b[data] == 4):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='y', marker='o' )\n",
    "        \n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (Y_train_one[data] == 1):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='r', marker='o' )\n",
    "        \n",
    "    elif (Y_train_one[data] == 2):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 3):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 4):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='y', marker='o' )\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 2b\",fontsize=10)\n",
    "        \n",
    "    \n",
    "plt.subplot(133)\n",
    "\n",
    "length_of_dataset_one = len(X_test_one)\n",
    "\n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (y_predicted21c[data] == 1):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='r', marker='o' )\n",
    "    \n",
    "    elif (y_predicted21c[data] == 2):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (y_predicted21c[data] == 3):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (y_predicted21c[data] == 4):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='y', marker='o' )\n",
    "        \n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (Y_train_one[data] == 1):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='r', marker='o' )\n",
    "        \n",
    "    elif (Y_train_one[data] == 2):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 3):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 4):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='y', marker='o' )\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 2c\",fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "with np.load('dataset2_2.npz') as data:\n",
    "    X_train_one = data['arr_0']\n",
    "    Y_train_one = data['arr_1']\n",
    "    X_test_one = data['arr_2']\n",
    "    Y_test_one = data['arr_3']\n",
    "\n",
    "y_predicted22a = Bayes2a(X_train_one,Y_train_one,X_test_one)\n",
    "y_predicted22b = Bayes2b(X_train_one,Y_train_one,X_test_one)\n",
    "y_predicted22c = Bayes2c(X_train_one,Y_train_one,X_test_one)\n",
    "\n",
    "\n",
    "plt.figure(4,figsize=(10,5))\n",
    "\n",
    "plt.subplot(131)\n",
    "\n",
    "length_of_dataset_one = len(X_test_one)\n",
    "\n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (y_predicted22a[data] == 1):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='r', marker='o' )\n",
    "    \n",
    "    elif (y_predicted22a[data] == 2):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (y_predicted22a[data] == 3):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (y_predicted22a[data] == 4):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='y', marker='o' )\n",
    "        \n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (Y_train_one[data] == 1):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='r', marker='o' )\n",
    "        \n",
    "    elif (Y_train_one[data] == 2):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 3):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 4):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='y', marker='o' )\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 2a\",fontsize=10)\n",
    "        \n",
    "    \n",
    "    \n",
    "plt.subplot(132)\n",
    "\n",
    "length_of_dataset_one = len(X_test_one)\n",
    "\n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (y_predicted22b[data] == 1):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='r', marker='o' )\n",
    "    \n",
    "    elif (y_predicted22b[data] == 2):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (y_predicted22b[data] == 3):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (y_predicted22b[data] == 4):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='y', marker='o' )\n",
    "        \n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (Y_train_one[data] == 1):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='r', marker='o' )\n",
    "        \n",
    "    elif (Y_train_one[data] == 2):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 3):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 4):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='y', marker='o' )\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 2b\",fontsize=10)\n",
    "        \n",
    "    \n",
    "plt.subplot(133)\n",
    "\n",
    "length_of_dataset_one = len(X_test_one)\n",
    "\n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (y_predicted22c[data] == 1):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='r', marker='o' )\n",
    "    \n",
    "    elif (y_predicted22c[data] == 2):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (y_predicted22c[data] == 3):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (y_predicted22c[data] == 4):\n",
    "        plt.scatter(X_test_one[data][0], X_test_one[data][1], c='y', marker='o' )\n",
    "        \n",
    "for data in range(length_of_dataset_one):\n",
    "    \n",
    "    if (Y_train_one[data] == 1):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='r', marker='o' )\n",
    "        \n",
    "    elif (Y_train_one[data] == 2):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='g', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 3):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='b', marker='o' )\n",
    "    \n",
    "    elif (Y_train_one[data] == 4):\n",
    "        plt.scatter(X_train_one[data][0], X_train_one[data][1], c='y', marker='o' )\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"x1\",fontsize=8)\n",
    "\n",
    "plt.ylabel(\"x2\", fontsize=8)\n",
    "\n",
    "plt.title(\"classifier 2c\",fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextWrite ** \n",
    "(Write your observations and table of errors here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead **\n",
    "\n",
    "# Problem 3 : Bias-Variance analysis in regression\n",
    "\n",
    "Do bias variance analysis for the following setting: \n",
    "\n",
    "$X \\sim Unif([-1,1]\\times[-1,1])$\n",
    "\n",
    "$Y=\\exp(-4*||X-a||^2) + \\exp(-4*||X-b||^2) + \\exp(-4*||X-c||^2)$\n",
    "\n",
    "where $a=[0.5,0.5], b=[-0.5,0.5], c=[0.5, -0.5]$.\n",
    "\n",
    "Regularised Risk = $\\frac{1}{m} \\sum_{i=1}^m (w^\\top \\phi(x_i) - y_i)^2 + \\frac{\\lambda}{2} ||w||^2 $ \n",
    "\n",
    "Sample 50 (X,Y) points from above distribution, and do ridge regularised polynomial regression with degrees=[1,2,4,8,16] and regularisation parameters ($\\lambda$) = [1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1e1]. Repeat for 100 times, and estimate the bias and variance for all 15 algorithms. You may approximate the distribution over X by discretising the $[-1,1]\\times[-1,1]$ space into 10000 points. (Both expectations over S and (x,y) are simply estimates due to the finiteness of our experiments and sample)\n",
    " \n",
    "3a) For each of the 30 algorithms (corresponding to 5 degrees and 6 lambda values) analyse the contour plot of the estimated $f_S$ for 3 different training sets. And the average $g(x) = E_S [f_S(x)]$. Write one function for doing everything in the code cell below. \n",
    "\n",
    "3b) In the next text cell, give the Bias and Variance computed as a $5\\times 6$ matrix, appropriately label the rows and columns. And give your conclusion in one or two sentences. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visualise 2d data [array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]]), array([[-1.        , -1.        , -1.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [-0.97979798, -0.97979798, -0.97979798, ..., -0.97979798,\n",
      "        -0.97979798, -0.97979798],\n",
      "       [-0.95959596, -0.95959596, -0.95959596, ..., -0.95959596,\n",
      "        -0.95959596, -0.95959596],\n",
      "       ...,\n",
      "       [ 0.95959596,  0.95959596,  0.95959596, ...,  0.95959596,\n",
      "         0.95959596,  0.95959596],\n",
      "       [ 0.97979798,  0.97979798,  0.97979798, ...,  0.97979798,\n",
      "         0.97979798,  0.97979798],\n",
      "       [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
      "         1.        ,  1.        ]]), array([[-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n",
      "         0.97979798,  1.        ],\n",
      "       [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n",
      "         0.97979798,  1.        ],\n",
      "       [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n",
      "         0.97979798,  1.        ],\n",
      "       ...,\n",
      "       [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n",
      "         0.97979798,  1.        ],\n",
      "       [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n",
      "         0.97979798,  1.        ],\n",
      "       [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n",
      "         0.97979798,  1.        ]]), array([[1.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
      "        1.        ],\n",
      "       [0.96000408, 0.96000408, 0.96000408, ..., 0.96000408, 0.96000408,\n",
      "        0.96000408],\n",
      "       [0.92082441, 0.92082441, 0.92082441, ..., 0.92082441, 0.92082441,\n",
      "        0.92082441],\n",
      "       ...,\n",
      "       [0.92082441, 0.92082441, 0.92082441, ..., 0.92082441, 0.92082441,\n",
      "        0.92082441],\n",
      "       [0.96000408, 0.96000408, 0.96000408, ..., 0.96000408, 0.96000408,\n",
      "        0.96000408],\n",
      "       [1.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
      "        1.        ]]), array([[ 1.        ,  0.97979798,  0.95959596, ..., -0.95959596,\n",
      "        -0.97979798, -1.        ],\n",
      "       [ 0.97979798,  0.96000408,  0.94021018, ..., -0.94021018,\n",
      "        -0.96000408, -0.97979798],\n",
      "       [ 0.95959596,  0.94021018,  0.92082441, ..., -0.92082441,\n",
      "        -0.94021018, -0.95959596],\n",
      "       ...,\n",
      "       [-0.95959596, -0.94021018, -0.92082441, ...,  0.92082441,\n",
      "         0.94021018,  0.95959596],\n",
      "       [-0.97979798, -0.96000408, -0.94021018, ...,  0.94021018,\n",
      "         0.96000408,  0.97979798],\n",
      "       [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n",
      "         0.97979798,  1.        ]]), array([[1.        , 0.96000408, 0.92082441, ..., 0.92082441, 0.96000408,\n",
      "        1.        ],\n",
      "       [1.        , 0.96000408, 0.92082441, ..., 0.92082441, 0.96000408,\n",
      "        1.        ],\n",
      "       [1.        , 0.96000408, 0.92082441, ..., 0.92082441, 0.96000408,\n",
      "        1.        ],\n",
      "       ...,\n",
      "       [1.        , 0.96000408, 0.92082441, ..., 0.92082441, 0.96000408,\n",
      "        1.        ],\n",
      "       [1.        , 0.96000408, 0.92082441, ..., 0.92082441, 0.96000408,\n",
      "        1.        ],\n",
      "       [1.        , 0.96000408, 0.92082441, ..., 0.92082441, 0.96000408,\n",
      "        1.        ]])]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-57811181fb9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;31m#      for reg_param in [1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1e1]:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;31m#         plt.figure()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_BV_error_sample_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'================================'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Degree= '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' lambda= '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-136-57811181fb9a>\u001b[0m in \u001b[0;36mcompute_BV_error_sample_plot\u001b[1;34m(degree, reg_param, num_training_samples)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;31m#print(weight_list_for_100_samples)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[0mg_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_list_for_100_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[0mvisualise_polynomial_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m    \u001b[1;31m# print(g_x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-136-57811181fb9a>\u001b[0m in \u001b[0;36mvisualise_polynomial_2d\u001b[1;34m(wt_vector, degree, title)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontourf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'learned function : degree= '\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell type : CodeWrite\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "coordinates_list = []\n",
    "def generate_X_train():\n",
    "    global coordinates_list\n",
    "    x = np.linspace(-1,1,10)   \n",
    "    for xcord in x :\n",
    "        for ycord in x:\n",
    "            a= [xcord,ycord]\n",
    "            coordinates_list.append(a)\n",
    "            \n",
    "generate_X_train()\n",
    "def polynomial_regression_ridge_pred(X_test, wt_vector, degree=1):\n",
    "    \"\"\" Give the value of the learned polynomial function, on test data.\n",
    "\n",
    "    Arguments:\n",
    "    X_test: numpy array of shape (n,d)\n",
    "    wt_vec: numpy array of shape (d',)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : numpy array of shape (n,)\n",
    "    \n",
    "    \"\"\"\n",
    "    Y_test_pred = []\n",
    "    feature_mapped_X_test = generate_feature_mapped_data(X_test,degree)\n",
    "    for test_data in X_test:\n",
    "        Y_test_pred.append(np.dot(np.array(test_data), wt_vector))\n",
    "    return Y_test_pred\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def visualise_polynomial_2d(wt_vector, degree, title=\"\"):\n",
    "    \"\"\"\n",
    "    Give a contour plot over the 2d-data domain for the learned polynomial given by the weight\n",
    "     vector wt_vector.\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "   X,Y = np.meshgrid(np.linspace(-1,1,100), np.linspace(-1,1,100))\n",
    "\n",
    "    z = np.random.rand(100,100)\n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            feature_mapped_data = feature_mapping_function(X[i][j],Y[i][j],degree)\n",
    "            z[i][j] = np.dot(wt_vector,feature_mapped_data)\n",
    "            \n",
    "\n",
    "    plt.contourf(X,Y,z,levels=np.linspace(0.,1.2 , 20))\n",
    "    plt.title('learned function : degree= '+ str(degree) + title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "def generate_feature_mapped_data(X_train,degree):\n",
    "    feature_mapped_X_data = []\n",
    "    for data in X_train:\n",
    "        feature_mapped_data = feature_mapping_function(data[0],data[1],degree)\n",
    "        feature_mapped_X_data.append(feature_mapped_data)\n",
    "    return feature_mapped_X_data   \n",
    "    \n",
    "def feature_mapping_function(x1,x2,degree):\n",
    "        feature_mapped_vector = []\n",
    "        for i in range(degree+1):\n",
    "            for j in range(i+1):\n",
    "                feature_mapped_vector.append((x1**j)*x2**(i-j))\n",
    "        return feature_mapped_vector\n",
    "\n",
    "\n",
    "def sample_X_train():\n",
    "    global coordinates_list\n",
    "    return random.sample(coordinates_list, 50)\n",
    "def sample_Y_train(sample_points):\n",
    "   \n",
    "    Y_train = []\n",
    "    for points in sample_points:\n",
    "        Y_train.append(y_value(points))\n",
    "    return Y_train\n",
    "\n",
    "def y_value(points):\n",
    "    a =[0.5,0.5]\n",
    "    b = [-0.5,0.5]\n",
    "    c = [0.5,-0.5]\n",
    "    #sub = np.subtract(points,a)\n",
    "    x = LA.norm(np.asmatrix(np.subtract(points,a)), 'fro')\n",
    "    y = LA.norm(np.asmatrix(np.subtract(points,b)), 'fro')\n",
    "    z = LA.norm(np.asmatrix(np.subtract(points,c)), 'fro')\n",
    "    value = np.exp(-4*(x)**2) + np.exp(-4*(y)**2) + np.exp(-4*(z)**2)\n",
    "\n",
    "    return value\n",
    "\n",
    "def polynomial_regression_ridge_train(X_train, Y_train, degree=1, reg_param=0.01):\n",
    "    \"\"\" Give best polynomial fitting data, based on empirical squared error minimisation.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: numpy array of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "    w : numpy array of shape (d',) with appropriate d'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    fo_identity_matrix = len(X_train[0])\n",
    "   \n",
    "    tranposed_data = np.array(X_train).transpose()\n",
    "    \n",
    "    inside_bracket =  np.linalg.inv( np.dot(tranposed_data,np.array(X_train)) + \\\n",
    "                      len(X_train)*reg_param*.05*np.identity(fo_identity_matrix))\n",
    "  \n",
    "\n",
    "    inverse_of_matrix = np.linalg.inv(inside_bracket) \n",
    "   \n",
    "    \n",
    "    xtrain_tarnpose_mul_y_train = np.dot(tranposed_data,Y_train) \n",
    "    w_hat = np.dot(inverse_of_matrix,xtrain_tarnpose_mul_y_train)\n",
    "    x = LA.norm(np.asmatrix(w_hat), 'fro')\n",
    "    \n",
    "    return w_hat/x\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def compute_BV_error_sample_plot(degree, reg_param, num_training_samples=50):\n",
    "    global coordinates_list\n",
    "    \n",
    "    weight_list_for_100_samples = []\n",
    "    for i in range(100):\n",
    "        sample_x_train = sample_X_train()\n",
    "        sampl_y_train = sample_Y_train(sample_x_train)\n",
    "        sample_featured_x_train = generate_feature_mapped_data(sample_x_train,degree)\n",
    "        \n",
    "        weight_list_for_100_samples.append(polynomial_regression_ridge_train\\\n",
    "                                               (sample_featured_x_train,sampl_y_train,\\\n",
    "                                                degree,reg_param))\n",
    "   \n",
    "    g_x = np.mean(weight_list_for_100_samples, axis=0)\n",
    "    visualise_polynomial_2d(g_x, degree, title=\"\")\n",
    "    \n",
    "\n",
    "    feature_mapped_Xtrain = generate_feature_mapped_data(coordinates_list,degree)\n",
    "    sum_for_each_weight = []\n",
    "    #variance finding\n",
    "    for weight in weight_list_for_100_samples :\n",
    "        sum =0\n",
    "        for co_ordinate in feature_mapped_Xtrain:\n",
    "            sum = sum + (np.dot(co_ordinate, weight) - np.dot(co_ordinate,g_x))**2\n",
    "        sum_for_each_weight.append(np.average(sum))\n",
    "    variance = np.average(sum_for_each_weight)\n",
    "    \n",
    "    #bias finding\n",
    "    bias = 0\n",
    "    \n",
    "    for co_ordinate in coordinates_list:\n",
    "        f = feature_mapping_function(co_ordinate[0],co_ordinate[1],\\\n",
    "                                                           degree)\n",
    "        #print(\"feature mapped\",f)\n",
    "        g = g_x\n",
    "        #print(\"g_x\",g)\n",
    "        \n",
    "        h = y_value(co_ordinate)\n",
    "        #print(\"y_value\",h)\n",
    "        \n",
    "        product = np.dot(g,f)\n",
    "        #print(\"np.dot(g,f)\",product)\n",
    "        bias = bias + (product-h)**2\n",
    "        #print(count)\n",
    "        #print(\"bias\",bias)\n",
    "        #print(\"********************************\")\n",
    "        \n",
    "    bias = bias/10000.0 \n",
    "    \n",
    "    #mean_square_error\n",
    "    mean_square_error = []\n",
    "    for weight in weight_list_for_100_samples :\n",
    "        sum =0\n",
    "        for co_ordinate in coordinates_list:\n",
    "            sum = sum + (np.dot(feature_mapping_function(co_ordinate[0],co_ordinate[1],\\\n",
    "                                                degree), weight) - y_value(co_ordinate))**2\n",
    "        mean_square_error.append(np.average(sum))\n",
    "    error = np.average(mean_square_error)\n",
    "        \n",
    "    return bias,variance,error\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \"\"\"Write code for generating data, fitting polynomial for given degree and reg_param. \n",
    "    Use num_training_samples samples for training.\n",
    "        \n",
    "    Compute the $f_S$ of 100 runs. \n",
    "\n",
    "    Plot 3 examples of learned function to illustrate how learned function varies \n",
    "    with different training samples. Also plot the average $f_S$ of all 100 runs.\n",
    "    \n",
    "    In total 4 subplots in one plot with appropriate title including degree and lambda value.\n",
    "    \n",
    "    Fill code to compute bias and variance, and average mean square error using the computed 100 $f_S$ functions.\n",
    "    \n",
    "    All contourplots are to be drawn with levels=np.linspace(0,1.2,20)\n",
    "    \n",
    "    Also return bias, variance, mean squared error. \"\"\"\n",
    "\n",
    "\n",
    "for degree in [1,2,4,8,16]:\n",
    "     for reg_param in [1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1e1]:\n",
    "        b,v,e = compute_BV_error_sample_plot(2, 1e-9)\n",
    "        print('================================')\n",
    "        print('Degree= '+str(2)+' lambda= '+str(1e-9))\n",
    "        print('Bias = '+str(b))\n",
    "        print('Variance = '+str(v))\n",
    "        print('MSE = '+str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell type: convenience\n",
    "# X_train_pos = np.random.randn(1000,2)+np.array([[1.,2.]])\n",
    "# X_train_neg = np.random.randn(1000,2)+np.array([[2.,4.]])\n",
    "# X_train = np.concatenate((X_train_pos, X_train_neg), axis=0)\n",
    "# Y_train = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))\n",
    "# X_test_pos = np.random.randn(1000,2)+np.array([[1.,2.]])\n",
    "# X_test_neg = np.random.randn(1000,2)+np.array([[2.,4.]])\n",
    "# X_test = np.concatenate((X_test_pos, X_test_neg), axis=0)\n",
    "# Y_test = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))\n",
    "# sample_x_train = sample_X_train()\n",
    "# #print(\"sample_x_train\",sample_x_train)\n",
    "# sampl_y_train = sample_Y_train(sample_x_train)\n",
    "# print(\"sampl_y_train\", sampl_y_train)\n",
    "# sample_featured_x_train = generate_feature_mapped_data(sample_x_train,1)\n",
    "# #print(\"sample_featured_x_train\",sample_featured_x_train)\n",
    "# polynomial_regression_ridge_train(sample_featured_x_train, sampl_y_train, degree=1, reg_param=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type: TextWrite **\n",
    "Give the biases and variances computed for the various algorithms with various degrees and lambdas and summarise your findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextRead **\n",
    "\n",
    "# Problem 4 : Analyse overfitting and underfitting in Regression\n",
    "\n",
    "\n",
    "Consider the 2-dimensional regression dataset \"dateset4_1.npz\". Do polynomial ridge regression for degrees = [1,2,4,8,16], and regularisation parameter $\\lambda$ = [1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1e1]. Do all the above by using three different subset sizes of the training set : 50, 100, 200 and 1000. (Just take the first few samples of X_train and Y_train.)\n",
    "\n",
    "Regularised Risk = $\\frac{1}{m} \\sum_{i=1}^m (w^\\top \\phi(x_i) - y_i)^2 + \\frac{\\lambda}{2} ||w||^2 $ \n",
    "\n",
    "The lambda value is given by the regularisation parameter. \n",
    "\n",
    "In the next codewrite cell, for each training set size compute how the train and test squared error varies with degree (by changing $\\phi$) and regularisation parameter (changing $\\lambda$). Compute the \"best\" degree and regularisation parameter based on the test squared error. Give a contour plot of the learned function for the chosen hyper-parameters, with appropriate title including the hyperparameters. Total number of figures = 4 (one for each training set size.)\n",
    "\n",
    "Summarise your findings in the next tex cell in a few sentences. And reproduce the tables showing train and test error for various training sizes, with appropriate row and column names.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8168239735973664\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9969201258378148\n",
      "0.8168239733854373\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9969201258449569\n",
      "0.8168239521925216\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9969201265590775\n",
      "0.8168218329669785\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9969201979198459\n",
      "0.8166105691692198\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.996926823433949\n",
      "0.8008057436903526\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9952458190193434\n",
      "1.051560479356989\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2184115828980415\n",
      "1.0515604786714385\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2184115823907122\n",
      "1.0515604101163278\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2184115316575916\n",
      "1.051553554657866\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2184064582768368\n",
      "1.0508685424114896\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2178984439471745\n",
      "0.9915345573267073\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.168565772859734\n",
      "1.1707834814345208\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.273767893800578\n",
      "1.1707834802622168\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2737678929604763\n",
      "1.1707833630318663\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2737678089498543\n",
      "1.1707716401723556\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2737594078816223\n",
      "1.1696011168483609\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2729192507046336\n",
      "1.0721730342608984\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.19618246535319\n",
      "1.2989182503717893\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2714717446965753\n",
      "1.2989182515034647\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2714717444544683\n",
      "1.2989181058339205\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2714716467246576\n",
      "1.298903506473111\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2714618509157654\n",
      "1.2974462747071949\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2704830982351092\n",
      "1.1779943364532086\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.1848459955226156\n",
      "1.4757237801731342\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2635391038323691\n",
      "1.4757239173018528\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.263539164635434\n",
      "1.47572377748216\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.263539081840451\n",
      "1.475709693830498\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2635307773552604\n",
      "1.4743036519722\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2627012094812766\n",
      "1.3563006397495454\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.1901805732522932\n",
      "results 1 10.0\n",
      "0.9234043333077837\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.988990577741425\n",
      "0.9234043332547214\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9889905777140194\n",
      "0.9234043279484557\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9889905749731237\n",
      "0.9234037973370124\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9889903008847839\n",
      "0.9233508869522551\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9889629052896171\n",
      "0.9193154045349436\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9865826426162996\n",
      "1.1752569880120527\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.206197970123437\n",
      "1.1752569872561893\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2061979693743554\n",
      "1.1752569116698877\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2061978944667628\n",
      "1.1752493529379777\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.206190403592931\n",
      "1.1744924853115792\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2054401999524453\n",
      "1.1012430787650322\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.132196121852337\n",
      "1.2972725209526856\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2458196945447126\n",
      "1.2972725195749222\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.245819693296703\n",
      "1.297272381798759\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2458195684964084\n",
      "1.297258604215906\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2458070884634225\n",
      "1.295881205144589\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2445590811391662\n",
      "1.1730351984947216\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.1315402915676143\n",
      "1.3671089799232847\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2198552914640035\n",
      "1.36710897803644\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2198552898680046\n",
      "1.3671087887973201\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2198551298496965\n",
      "1.3670898657822759\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2198391287019672\n",
      "1.3652005070252509\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.218241194496783\n",
      "1.2086974714053933\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0841988717238393\n",
      "1.3194582357402367\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.11982984883532\n",
      "1.3194584106794738\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.1198299667264116\n",
      "1.3194582511700126\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.1198298231690706\n",
      "1.3194423540540898\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.1198155917384804\n",
      "1.317855867433192\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.118395580266143\n",
      "1.189358776545232\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0046805243521744\n",
      "results 1 10.0\n",
      "1.0160452043439603\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0034837464699766\n",
      "1.0160452043115245\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0034837464294402\n",
      "1.0160452010679726\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0034837423757337\n",
      "1.0160448767207313\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0034833370171754\n",
      "1.016012521647251\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0034429172351544\n",
      "1.0134921461137438\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0003683247467148\n",
      "1.264625798138292\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2142561396961051\n",
      "1.2646257968137171\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.214256138322514\n",
      "1.2646256643561593\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2142560009640184\n",
      "1.2646124180642406\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.214242264635587\n",
      "1.2632825065484519\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2128639211074723\n",
      "1.1224727544258526\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0706166467397142\n",
      "1.336934197140121\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.243001535089036\n",
      "1.3369341946379925\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2430015326004795\n",
      "1.3369339444250108\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2430012837450648\n",
      "1.336908922868146\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2429763980360322\n",
      "1.3344042740192672\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2404862363756135\n",
      "1.1073706818592275\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0189964255671522\n",
      "1.309566482601374\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.1722456754654393\n",
      "1.3095664792779271\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.1722456722433516\n",
      "1.309566146707578\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.17224534984315\n",
      "1.3095328904649335\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.1722131107070535\n",
      "1.3062121644527318\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.1689951034611885\n",
      "1.0430967750142908\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9193211193122252\n",
      "1.1967233114761369\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0305529432282732\n",
      "1.1967232080022006\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.030552793212871\n",
      "1.1967229386394345\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0305525255808299\n",
      "1.1966959493308957\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0305257096927014\n",
      "1.1940034023952917\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0278522840279993\n",
      "0.9884267908506604\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.8321157001114105\n",
      "results 16 10.0\n",
      "1.0237083674918515\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.010314047941567\n",
      "1.0237083674836194\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0103140479394304\n",
      "1.0237083666605238\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0103140477253423\n",
      "1.023708284352569\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0103140263151904\n",
      "1.023700069643942\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0103118713429489\n",
      "1.0230414992320502\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.0100583481463894\n",
      "1.2543326312813903\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.232962833150603\n",
      "1.2543326298245976\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2329628317246533\n",
      "1.254332484145151\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2329626891292955\n",
      "1.2543179154396984\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.232948428844934\n",
      "1.25285354340216\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.231515016557575\n",
      "1.0926646233780073\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.074481531392669\n",
      "1.3219473847759544\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2915576945672231\n",
      "1.3219473818673937\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2915576917208653\n",
      "1.321947091011036\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2915574070843174\n",
      "1.3219180048616523\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2915289429357915\n",
      "1.3190043985242896\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.288677733615546\n",
      "1.0525602217374963\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.028307559535276\n",
      "1.3160222430107933\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2698504587458193\n",
      "1.3160222386708817\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2698504545156408\n",
      "1.3160218047940235\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2698500316041497\n",
      "1.3159784175457805\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2698077409216015\n",
      "1.3116457659327412\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2655850173859062\n",
      "0.9772886922476837\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.9415178822630181\n",
      "1.2686500192056445\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2080526200169366\n",
      "1.2686500216717183\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2080526223993837\n",
      "1.2686495667487359\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2080521845708314\n",
      "1.2686041231462015\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2080084446091615\n",
      "1.264073816988655\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "1.2036486727037168\n",
      "0.9421752824034612\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "0.8968181148238784\n",
      "results 16 10.0\n"
     ]
    }
   ],
   "source": [
    "# Cell type : CodeWrite \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "with np.load('dataset4_1.npz') as data:\n",
    "    X_train = data['arr_0']\n",
    "    Y_train = data['arr_1']\n",
    "    X_test = data['arr_2']\n",
    "    Y_test = data['arr_3']\n",
    "    \n",
    "def train_error(w_vector,sample_x_train,sampl_y_train):\n",
    "    total_error = 0\n",
    "    for i in range(len(sample_x_train)):\n",
    "        total_error = total_error + (np.dot(w_vector,sample_x_train[i]) - sampl_y_train[i])**2\n",
    "    return float(total_error)/len(sample_x_train)\n",
    "\n",
    "def test_error(w_vector, sample_x_test, sampl_y_test):\n",
    "    total_error = 0\n",
    "    for i in range(len(sample_x_test)):\n",
    "        total_error = total_error + (np.dot(w_vector,sample_x_test[i]) - sampl_y_test[i])**2\n",
    "    return float(total_error)/len(sample_x_test)\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "minimum_test_error\n",
    "best_degree\n",
    "best_reg_param \n",
    "def plot_the_graph_for_points(sample_points):\n",
    "    global minimum_test_error \n",
    "    minimum_test_error = 2323432433443\n",
    "    global best_degree\n",
    "    global best_reg_param\n",
    "    global Y_test\n",
    "    \n",
    "    for degree in [1,2,4,8,16]:\n",
    "        for reg_param in [1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1e1]:\n",
    "\n",
    "            sample_x_train = X_train[0:sample_points,:]\n",
    "            sampl_y_train = Y_train[:sample_points]\n",
    "\n",
    "            sample_featured_x_train = generate_feature_mapped_data(sample_x_train,degree)\n",
    "            w_vector = polynomial_regression_ridge_train(sample_featured_x_train, sampl_y_train, degree, reg_param)\n",
    "\n",
    "            trained_error = train_error(w_vector,sample_featured_x_train,sampl_y_train)\n",
    "            print(trained_error)\n",
    "\n",
    "            sample_featured_x_test = generate_feature_mapped_data(X_test,degree)\n",
    "            print(\"&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\")\n",
    "            tested_error = test_error(w_vector, sample_featured_x_test, Y_test )\n",
    "            print(tested_error)\n",
    "            #print(degree ,  reg_param,  trained_error,  tested_error )\n",
    "            #print(minimum_test_error)\n",
    "\n",
    "            if tested_error<minimum_test_error :\n",
    "                minimum_test_error = tested_error\n",
    "                best_degree = degree\n",
    "                best_reg_param = reg_param\n",
    "    print(\"results\",best_degree,best_reg_param)\n",
    "\n",
    "def plotting_the_graph():\n",
    "    #for size 50\n",
    "    plot_the_graph_for_points(50)\n",
    "    \n",
    "    #for size 100\n",
    "    plot_the_graph_for_points(100)\n",
    "    \n",
    "    #for size 200\n",
    "    plot_the_graph_for_points(200)\n",
    "    \n",
    "    #for size 1000\n",
    "    plot_the_graph_for_points(1000)\n",
    "    \n",
    "plotting_the_graph()\n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextWrite **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
